#include "kittens.cuh"
// #include "pyutils/pyutils.cuh"
#include <fstream>
#include <chrono>
#include <cmath>

#define DEBUG

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

// #ifndef ATTN_B
// constexpr int ATTN_B = 16; // batch size
// #endif

// #ifndef ATTN_H
// constexpr int ATTN_H = 8;  // number of heads
// #endif

// #ifndef ATTN_D
// constexpr int ATTN_D = 128; // dimension
// #endif

// #ifndef ATTN_F
// constexpr int ATTN_F = 128;  // number of features
// #endif

// #ifndef ATTN_N
// constexpr int ATTN_N = 1024; // sequence length
// #endif

// constexpr int CHUNK_SIZE = 64;

// debug
#ifndef ATTN_B
constexpr int ATTN_B = 1;//16; // batch size
#endif
#ifndef ATTN_H
constexpr int ATTN_H = 1;  // number of heads
#endif
#ifndef ATTN_D
constexpr int ATTN_D__128 = 128; // dimension
#endif
#ifndef ATTN_F
constexpr int ATTN_F__128 = 128;  // number of features
#endif
#ifndef ATTN_N
constexpr int ATTN_N = 64;//1024; // sequence length
#endif
constexpr int CHUNK_SIZE__64 = 64;

using namespace kittens;

using G = kittens::group<NUM_WARPS>;

// template<int ATTN_F, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using q_tile = rt<T, CHUNK_SIZE, ATTN_F, L, S>;
// template<int ATTN_F, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using q_tile_transposed = rt<T, ATTN_F, CHUNK_SIZE, L, S>;
// template<int ATTN_F, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using k_tile = rt<T, CHUNK_SIZE, ATTN_F, L, S>;
// template<int ATTN_D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using v_tile = rt<T, CHUNK_SIZE, ATTN_D, L, S>;
// template<int ATTN_F, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using k_tile_transposed = rt<T, ATTN_F, CHUNK_SIZE, L, S>;
// template<int ATTN_D, typename T=float, typename L=col_l, typename S=rt_16x32_4_s> using attn_tile = rt<T, CHUNK_SIZE, CHUNK_SIZE, L, S>;
// template<int ATTN_D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using o_tile = rt<T, CHUNK_SIZE, ATTN_D, L, S>;
// template<int ATTN_D, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using o_tile_transposed = rt<T, ATTN_D, CHUNK_SIZE, L, S>;
template<int F, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using q_tile = rt<T, CHUNK_SIZE__64, F, L, S>;
template<int F, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using q_tile_transposed = rt<T, F, CHUNK_SIZE__64, L, S>;
template<int F, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using k_tile = rt<T, CHUNK_SIZE__64, F, L, S>;
template<int D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using v_tile = rt<T, CHUNK_SIZE__64, D, L, S>;
template<int F, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using k_tile_transposed = rt<T, F, CHUNK_SIZE__64, L, S>;
template<int D, typename T=float, typename L=col_l, typename S=rt_16x32_4_s> using attn_tile = rt<T, CHUNK_SIZE__64, CHUNK_SIZE__64, L, S>;
template<int D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s> using o_tile = rt<T, CHUNK_SIZE__64, D, L, S>;
template<int D, typename T=bf16, typename L=col_l, typename S=rt_16x32_s> using o_tile_transposed = rt<T, D, CHUNK_SIZE__64, L, S>;
template<int D, typename T=float, typename L=col_l, typename S=rt_16x32_s> using kv_state_tile = rt<T, ATTN_F__128, ATTN_D__128, L, S>;

using _gl_QKVO = gl<bf16, -1, -1, -1, -1>;

struct lightning_attn2_globals {
    // // shapes    
    // using q_tile       = st_bf<CHUNK_SIZE, ATTN_F>;
    // using k_tile       = st_bf<CHUNK_SIZE, ATTN_F>;
    // using k_tile_split = st_bf<CHUNK_SIZE, ATTN_F/2>;
    // using v_tile       = st_bf<CHUNK_SIZE, ATTN_D>;
    // using o_tile       = st_bf<CHUNK_SIZE, ATTN_D>;

    // // global layouts
    // using q_gl       = gl<bf16,  -1, -1, -1, -1, q_tile>; // TODO: gl<bf16,  -1, -1, -1, -1>; No TMA types
    // using k_gl       = gl<bf16,  -1, -1, -1, -1, k_tile>;
    // using k_split_gl = gl<bf16,  -1, -1, -1, -1, k_tile_split>;
    // using v_gl       = gl<bf16,  -1, -1, -1, -1, v_tile>;
    // using o_gl       = gl<bf16,  -1, -1, -1, -1, o_tile>;

    // shapes    
    // using q_tile       = st_bf<CHUNK_SIZE, ATTN_F>;
    // using k_tile       = st_bf<CHUNK_SIZE, ATTN_F>;
    // using k_tile_split = st_bf<CHUNK_SIZE, ATTN_F/2>;
    // using v_tile       = st_bf<CHUNK_SIZE, ATTN_D>;
    // using o_tile       = st_bf<CHUNK_SIZE, ATTN_D>;

    // global layouts
    // using q_gl       = gl<bf16,  -1, -1, -1, -1>; // TODO: gl<bf16,  -1, -1, -1, -1>; No TMA types
    // using k_gl       = gl<bf16,  -1, -1, -1, -1>;
    // using k_split_gl = gl<bf16,  -1, -1, -1, -1>;
    // using v_gl       = gl<bf16,  -1, -1, -1, -1>;
    // using o_gl       = gl<bf16,  -1, -1, -1, -1>;

    // q_gl Qg;
    // k_gl Kg;
    // k_split_gl K_split_g;
    // v_gl Vg;
    // o_gl Og;

    _gl_QKVO Qg, Kg, K_split_g, Vg, Og;

    _gl_QKVO ODEBUGg;

    // float *slopes;
    uintptr_t slopes;

    hipStream_t stream;
    // dim3 grid() {return dim3(ATTN_B, ATTN_H);}
    // dim3 block() {return dim3(ATTN_F, 1, 1);}
    dim3 block() {return dim3(NUM_THREADS);}
    // dim3 grid() { return dim3(ATTN_H, ((ATTN_N / CHUNK_SIZE + NUM_WARPS - 1) / NUM_WARPS), ATTN_B); }
    dim3 grid() {return dim3(ATTN_H, ATTN_B);}
    // dim3 grid() {return dim3(ATTN_B, ATTN_H);}
    size_t dynamic_shared_memory() { return MAX_SHARED_MEMORY; }
};

__device__ static inline void dump_bits(bf16 * val, const int line_num) {
    float val_f = float(val[0]);
    uint16_t val_bits = *reinterpret_cast<uint16_t*>(val);

    // 快速打印核心信息
    printf("%d float value: %f → 十六进制: 0x%04X → 二进制：", line_num, val_f, val_bits);
    // 快速输出二进制字符串（16位）
    for (int i = 15; i >= 0; i--) {
        printf("%d", (val_bits >> i) & 1);
        if ((i % 8) == 7) printf(" "); // 每8位加空格，便于阅读
    }
    printf("\n");
    // bool is_all_zero = (val_bits == 0); // 所有bit位全0 → true
    // if (!is_all_zero) {
    //     printf("non-zero value found.\n");
    // }
}

__device__ static inline void wg_arange(auto &vec) {
    #pragma unroll
    for (int i = 0; i < vec.length; i++) {
        // float val = static_cast<float>(i) + (warpid() * vec.length); 
        float val = static_cast<float>(i);
        vec.data[i] = val; 
    }
    // group<4>::sync(5 + warpgroupid());
}
// __device__ static inline void wg_arange(auto &vec) {
//     // #pragma unroll
//     // for (int i = 0; i < vec.length; i++) {
//     //     float val = static_cast<float>(i) + (warpid() * vec.length); 
//     //     vec.data[i] = val; 
//     // }
//     if (threadIdx.x < CHUNK_SIZE)
//         vec.data[threadIdx.x] = threadIdx.x;
// }

__device__ static inline float get_scale(int i, int j, float slope) {
    // float ret = __expf(-(i-j) * slope);
    // if (i < j) ret *= 0;

    // As attn_block is Q^TK, not QK^T, let's exchange i and j.
    float ret = __expf(-(j-i) * slope);
    if (j < i) ret *= 0;
        return ret;
}

template<ducks::rt::row_layout RT>
__device__ static inline void apply_mask(RT &dst, const RT &src, float slope) {

}

// __global__ __launch_bounds__(NUM_THREADS, 1)
__global__ __launch_bounds__(NUM_THREADS, 2)
void lightning_attn2_kernel(const lightning_attn2_globals globals, int N)
{
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);

    // smem
    st_bf<CHUNK_SIZE__64, ATTN_F__128, st_32x32_s> (&q_smem)[2] = al.allocate<st_bf<CHUNK_SIZE__64, ATTN_F__128, st_32x32_s>, 2>();
    st_bf<CHUNK_SIZE__64, ATTN_F__128, st_32x32_s> (&k_smem)[2] = al.allocate<st_bf<CHUNK_SIZE__64, ATTN_F__128, st_32x32_s>, 2>();
    st_bf<CHUNK_SIZE__64, ATTN_D__128, st_32x32_s> (&v_smem)[2] = al.allocate<st_bf<CHUNK_SIZE__64, ATTN_D__128, st_32x32_s>, 2>();
    
    st_bf<ATTN_F__128, ATTN_D__128, st_32x32_s> (&kv_state_smem) = al.allocate<st_bf<ATTN_F__128, ATTN_D__128, st_32x32_s>>();


    row_vec<st_fl<ATTN_D__128, CHUNK_SIZE__64, st_32x32_s>> (&q_decay) = al.allocate<row_vec<st_fl<ATTN_D__128, CHUNK_SIZE__64, st_32x32_s>>>();
    row_vec<st_fl<ATTN_D__128, CHUNK_SIZE__64, st_32x32_s>> (&k_decay) = al.allocate<row_vec<st_fl<ATTN_D__128, CHUNK_SIZE__64, st_32x32_s>>>();
    // decay in register
    constexpr int sizeof_shared__131584 = sizeof(q_smem) + sizeof(k_smem) + sizeof(v_smem) + sizeof(kv_state_smem) + sizeof(q_decay) + sizeof(k_decay);// + sizeof(kv_state_smem222);
    static_assert(sizeof_shared__131584 < 160000);
    row_vec<rt_fl<ATTN_D__128, CHUNK_SIZE__64, col_l, rt_32x32_s>> q_decay_rv;
    col_vec<rt_fl<CHUNK_SIZE__64, ATTN_D__128, col_l, rt_32x32_s>> k_decay_rv;

    const int head_idx = blockIdx.x;
    const int batch_idx = blockIdx.y;
    float slope = reinterpret_cast<float*>(globals.slopes)[head_idx];
    // printf("head_idx %d batch_idx %d slope %f\n", head_idx, batch_idx, slope);


    int blocks = N / CHUNK_SIZE__64;
    // printf("blocks: %d\n", blocks);

    const int tic = 0, toc = 1;

    // Initialize all of the register tiles.
    q_tile<ATTN_F__128, bf16> q_reg;                         // [CHUNK_SIZE, ATTN_F], 64x128
    q_tile<ATTN_F__128, bf16> q_reg_copy, q_reg_copy2;                         // [CHUNK_SIZE, ATTN_F], 64x128
    // q_tile_transposed<ATTN_F, bf16> q_reg_transposed;   // [ATTN_F, CHUNK_SIZE], 128x64, rt_16x32_s, 8x2 subtiles
    // k_tile<ATTN_F, bf16> k_reg;                         // [CHUNK_SIZE, ATTN_F], 64x128
    // k_tile_transposed<ATTN_F, bf16> k_reg_transposed;   // [ATTN_F, CHUNK_SIZE], 128x64
    
    // v_tile<ATTN_D, bf16, col_l, rt_32x32_s> v_reg;                    // [CHUNK_SIZE, ATTN_D], 64x128
    // o_tile_transposed<ATTN_D, float, col_l, rt_32x32_s> o_reg;          // [ATTN_D, CHUNK_SIZE], 128x64
    // // o_tile<ATTN_D, float, col_l, rt_32x32_s> o_inter;                   // [CHUNK_SIZE, ATTN_D]
    // o_tile_transposed<ATTN_D, float, col_l, rt_32x32_s> o_inter; // [ATTN_D, CHUNK_SIZE]
    // attn_tile<ATTN_D, float, col_l, rt_32x32_s> attn_block[2];          // [CHUNK_SIZE, CHUNK_SIZE], 64x64
    // attn_tile<ATTN_D, bf16, col_l, rt_32x32_s> attn_block_bf16;         // [CHUNK_SIZE, CHUNK_SIZE], 64x64
    // attn_tile<ATTN_D, bf16, col_l, rt_16x32_4_s> attn_block_bf16_in;    // [64x64], 内部16x32

    //template<int D, typename T=float, typename L=col_l, typename S=rt_16x32_s> using kv_state_tile = rt<T, ATTN_F, ATTN_D, L, S>;
    rt_bf<ATTN_F__128, ATTN_D__128, col_l, rt_16x32_s> local_kv_reg; // [ATTN_F, ATTN_D], 8x4 subtiles


    
    // using T = typename st_bf<CHUNK_SIZE, ATTN_F, st_32x32_s>::dtype;
    // constexpr int bytes_per_thread = st_32x32_s::template bytes_per_thread<T>();
    // constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
    // constexpr int memcpy_per_tile_q_k = CHUNK_SIZE * ATTN_F * sizeof(T) / bytes_per_memcpy;
    // constexpr int memcpy_per_tile_v = CHUNK_SIZE * ATTN_D * sizeof(T) / bytes_per_memcpy;
    // uint32_t swizzled_offsets_Q[memcpy_per_tile_q_k];
    // uint32_t swizzled_offsets_V[memcpy_per_tile_v];
    // uint32_t swizzled_offsets_K[memcpy_per_tile_q_k];
    // G::prefill_swizzled_offsets<1, false>(q_smem[0], globals.Qg, swizzled_offsets_Q);
    // G::prefill_swizzled_offsets<1, false>(k_smem[0], globals.Kg, swizzled_offsets_K);
    // G::prefill_swizzled_offsets<1, false>(v_smem[0], globals.Vg, swizzled_offsets_V);

#ifdef DEBUG
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        for (int i = 0; i < 8; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("q_smem[0] value before init, should be random. %f\n", float(q_smem[0].data[i]));
                dump_bits(&q_smem[0].data[i], __LINE__);
            }
        }
        
    }
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        if (threadIdx.x == 0 && threadIdx.y == 0) {
            float q_smem_sum = 0.0f;
            for (int i = 0; i < ATTN_F__128 * CHUNK_SIZE__64; i++) {
                q_smem_sum += float(q_smem[0].data[i]);
            }
            printf("before init, q_smem_sum %f\n\n", q_smem_sum);
        }
    }
    __syncthreads();
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);
#endif

    ones(q_reg);
    copy(q_reg_copy, q_reg);
    store(q_smem[0], q_reg);
    asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);
#ifdef DEBUG
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        // tile[0][0]
        for (int i = 0; i < 4; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("q_reg height %d width %d\n", q_reg.height, q_reg.width); // 8, 4
                printf("q_reg.tiles[0][0].packed_per_thread %d\n", q_reg.tiles[0][0].packed_per_thread); // 4
                float temp = __bfloat162float((q_reg.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
                printf("q_reg.tiles[0][0].data[%d].x: %f\n", i, temp);
                dump_bits(&q_reg.tiles[0][0].data[i].x, __LINE__);
                temp = __bfloat162float((q_reg.tiles[0][0].data[i].y));
                printf("q_reg.tiles[0][0].data[%d].y: %f\n", i, temp);
                dump_bits(&q_reg.tiles[0][0].data[i].y, __LINE__);
            }
        }
        for (int i = 0; i < 4; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("q_reg_copy height %d width %d\n", q_reg_copy.height, q_reg_copy.width); // 8, 4
                printf("q_reg_copy.tiles[0][0].packed_per_thread %d\n", q_reg_copy.tiles[0][0].packed_per_thread); // 4
                float temp = __bfloat162float((q_reg_copy.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
                printf("q_reg_copy.tiles[0][0].data[%d].x: %f\n", i, temp);
                dump_bits(&q_reg_copy.tiles[0][0].data[i].x, __LINE__);
                temp = __bfloat162float((q_reg_copy.tiles[0][0].data[i].y));
                printf("q_reg_copy.tiles[0][0].data[%d].y: %f\n", i, temp);
                dump_bits(&q_reg_copy.tiles[0][0].data[i].y, __LINE__);
            }
        }
        for (int i = 0; i < 8; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("q_smem[0] value should be 1 here %f\n", float(q_smem[0].data[i]));
                dump_bits(&q_smem[0].data[i], __LINE__);
            }
        }
    }
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        if (threadIdx.x == 0 && threadIdx.y == 0) {
            float q_smem_sum = 0.0f;
            for (int i = 0; i < ATTN_F__128 * CHUNK_SIZE__64; i++) {
                q_smem_sum += float(q_smem[0].data[i]);
            }
            printf("after init, q_smem_sum %f\n\n", q_smem_sum);
        }
    }
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);
#endif
    
    // load from st to rt back
//     load(q_reg, q_smem[0]);
//     asm volatile("s_waitcnt lgkmcnt(0)");
//     asm volatile("s_waitcnt vmcnt(0)");
//     __builtin_amdgcn_sched_barrier(0);
//     __builtin_amdgcn_s_barrier();
//     __builtin_amdgcn_sched_barrier(0);
// #ifdef DEBUG
//     if (blockIdx.x == 0 && blockIdx.y == 0) {
//         // tile[0][0]
//         for (int i = 0; i < 4; i++) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 printf("q_reg height %d width %d\n", q_reg.height, q_reg.width); // 8, 4
//                 printf("q_reg.tiles[0][0].data length: %zu\n", sizeof(q_reg.tiles[0][0].data) / sizeof(float));
//                 float temp = __bfloat162float((q_reg.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
//                 printf("q_reg.tiles[0][0].data[%d].x: %f\n", i, temp);
//                 // uint16_t val_bits = *reinterpret_cast<uint16_t*>(&q_reg.tiles[0][0].data[i].x);
//                 dump_bits(&q_reg.tiles[0][0].data[i].x);
//                 temp = __bfloat162float((q_reg.tiles[0][0].data[i].y));
//                 printf("q_reg.tiles[0][0].data[%d].y: %f\n", i, temp);
//                 dump_bits(&q_reg.tiles[0][0].data[i].y);
//             }
//         }
//         for (int i = 0; i < 8; i++) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 printf("q_smem[0] value %f\n", float(q_smem[0].data[i]));
//                 dump_bits(&q_smem[0].data[i]);
//             }
//         }
//     }
    load(q_reg_copy2, q_smem[0]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(0)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);
#ifdef DEBUG
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        // tile[0][0]
        for (int i = 0; i < 4; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0) {
                printf("q_reg_copy2 height %d width %d\n", q_reg_copy2.height, q_reg_copy2.width); // 8, 4
                printf("q_reg_copy2.tiles[0][0].data length: %zu\n", sizeof(q_reg_copy2.tiles[0][0].data) / sizeof(float));
                float temp = __bfloat162float((q_reg_copy2.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
                printf("q_reg_copy2.tiles[0][0].data[%d].x: %f\n", i, temp);
                // uint16_t val_bits = *reinterpret_cast<uint16_t*>(&q_reg.tiles[0][0].data[i].x);
                dump_bits(&q_reg_copy2.tiles[0][0].data[i].x, __LINE__);
                temp = __bfloat162float((q_reg_copy2.tiles[0][0].data[i].y));
                printf("q_reg_copy2.tiles[0][0].data[%d].y: %f\n", i, temp);
                dump_bits(&q_reg_copy2.tiles[0][0].data[i].y, __LINE__);
            }
        }
        for (int i = 0; i < 8; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0) {
                printf("q_smem[0] value %f\n", float(q_smem[0].data[i]));
                dump_bits(&q_smem[0].data[i], __LINE__);
            }
        }
    }
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        if (threadIdx.x == 0 && threadIdx.y == 0) {
            float q_smem_sum = 0.0f;
            for (int i = 0; i < ATTN_F__128 * CHUNK_SIZE__64; i++) {
                q_smem_sum += float(q_smem[0].data[i]);
            }
            printf("after reload q_smem to q_reg, q_smem_sum %f\n\n", q_smem_sum);
        }
    }
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);      
#endif

    // zero(o_reg);
    // zero(kv_state_smem);
    ones(local_kv_reg);
    store(kv_state_smem, local_kv_reg);
    asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
    __syncthreads();
    auto now = sizeof(kv_state_smem);
    // if(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.y == 0 && blockIdx.x == 0 && blockIdx.y == 0){
    //     for(int i = 0; i < sizeof(kv_state_smem) / 2; i++) {
    //         dump_bits(kv_state_smem.data + i, __LINE__);
    //     }
    // }
    __syncthreads();
    __builtin_amdgcn_s_barrier();                      // block内全线程栅栏同步 (必加)
    
#ifdef DEBUG
    if (blockIdx.x == 0 && blockIdx.y == 0) {
        // tile[0][0]
        for (int i = 0; i < 4; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("local_kv_reg height %d width %d\n", local_kv_reg.height, local_kv_reg.width); // 8, 4
                printf("local_kv_reg.tiles[0][0].packed_per_thread %d\n", local_kv_reg.tiles[0][0].packed_per_thread); // 4
                float temp = __bfloat162float((local_kv_reg.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
                printf("local_kv_reg.tiles[0][0].data[%d].x: %f\n", i, temp);
                dump_bits(&local_kv_reg.tiles[0][0].data[i].x, __LINE__);
                temp = __bfloat162float((local_kv_reg.tiles[0][0].data[i].y));
                printf("local_kv_reg.tiles[0][0].data[%d].y: %f\n", i, temp);
                dump_bits(&local_kv_reg.tiles[0][0].data[i].y, __LINE__);
            }
        }
        for (int i = 0; i < 8; i++) {
            if (threadIdx.x == 0 && threadIdx.y == 0){
                printf("kv_state_smem %f\n", float(kv_state_smem.data[i]));
                dump_bits(&kv_state_smem.data[i], __LINE__);
            }
        }
    }
#endif
    
    // load from st to rt back
    asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
    load222(local_kv_reg, kv_state_smem);
    asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
    __syncthreads();
#ifdef DEBUG
        if (blockIdx.x == 0 && blockIdx.y == 0) {
            // tile[0][0]
            for (int i = 0; i < 4; i++) {
                if (threadIdx.x == 0 && threadIdx.y == 0) {
                    printf("local_kv_reg height %d width %d\n", local_kv_reg.height, local_kv_reg.width); // 8, 4
                    printf("local_kv_reg.tiles[0][0].data length: %zu\n", sizeof(local_kv_reg.tiles[0][0].data)/sizeof(float));
                    float temp = float((local_kv_reg.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
                    printf("local_kv_reg.tiles[0][0].data[%d].x: %f\n", i, temp);
                    // uint16_t val_bits = *reinterpret_cast<uint16_t*>(&local_kv_reg.tiles[0][0].data[i].x);
                    dump_bits(&local_kv_reg.tiles[0][0].data[i].x, __LINE__);
                    temp = float((local_kv_reg.tiles[0][0].data[i].y));
                    printf("local_kv_reg.tiles[0][0].data[%d].y: %f\n", i, temp);
                    dump_bits(&local_kv_reg.tiles[0][0].data[i].y, __LINE__);
                }
            }
            for (int i = 0; i < 8; i++) {
                if (threadIdx.x == 0 && threadIdx.y == 0){
                    printf("kv_state_smem %f\n", float(kv_state_smem.data[i]));
                    dump_bits(&kv_state_smem.data[i], __LINE__);
                }
            }
        }
#endif
    return;

//     using T = typename st_bf<CHUNK_SIZE, ATTN_F, st_32x32_s>::dtype;
//     constexpr int bytes_per_thread = st_32x32_s::template bytes_per_thread<T>();
//     constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
//     constexpr int memcpy_per_tile_q_k = CHUNK_SIZE * ATTN_F * sizeof(T) / bytes_per_memcpy;
//     constexpr int memcpy_per_tile_v = CHUNK_SIZE * ATTN_D * sizeof(T) / bytes_per_memcpy;
//     uint32_t swizzled_offsets_Q[memcpy_per_tile_q_k];
//     uint32_t swizzled_offsets_V[memcpy_per_tile_v];
//     uint32_t swizzled_offsets_K[memcpy_per_tile_q_k];
//     G::prefill_swizzled_offsets<1, false>(q_smem[0], globals.Qg, swizzled_offsets_Q);
//     G::prefill_swizzled_offsets<1, false>(k_smem[0], globals.Kg, swizzled_offsets_K);
//     G::prefill_swizzled_offsets<1, false>(v_smem[0], globals.Vg, swizzled_offsets_V);

//     int warpid = kittens::warpid();
//     if (warpid == 0) {
//         wg_arange(q_decay);
//         // warpgroup::mul(q_decay, q_decay, -1.0f * slope);
//         // warpgroup::exp(q_decay, q_decay);
//         // mul(q_decay, q_decay, -1.0f * slope);
//         // exp(q_decay, q_decay); // OPT: exp2 -> exp
//         asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
//         __builtin_amdgcn_s_barrier();                      // block内全线程栅栏同步 (必加)
        
//         load(q_decay_rv, q_decay);
//         // mul(q_decay_rv, q_decay_rv, -1.0f * slope);
//         // exp(q_decay_rv, q_decay_rv);
//         __builtin_amdgcn_sched_barrier(0);
//         asm volatile("s_waitcnt lgkmcnt(0)");
//         asm volatile("s_waitcnt vmcnt(0)");
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
// #ifdef DEBUG
//         // if (blockIdx.x == 0 && blockIdx.y == 0) {
//         if (blockIdx.x == 5 && blockIdx.y == 0) {
//             for (int i = 0; i < 8; i++) {
//                 if (threadIdx.x == 0 && threadIdx.y == 0) {
//                     printf("q_decay[%d] %f\n", i, q_decay.data[i]);
//                 }
//             }
//             for (int i = CHUNK_SIZE - 8; i < CHUNK_SIZE; i++) {
//                 if (threadIdx.x == 0 && threadIdx.y == 0) {
//                     printf("q_decay[%d] %f\n", i, q_decay.data[i]);
//                 }
//             }
//         }
//         // // if (blockIdx.x == 0 && blockIdx.y == 0) {
//         if (blockIdx.x == 5 && blockIdx.y == 0) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 printf("q_decay_rv outer_dim %d inner_dim %d elements_per_thread %d packing %d\n",
//                     q_decay_rv.outer_dim, q_decay_rv.inner_dim, q_decay_rv.elements_per_thread, q_decay_rv.packing); // 2, 1, 16, 1
//                 printf("slope %f\n", slope);
//             }
//         }
//         // if (blockIdx.x == 0 && blockIdx.y == 0) {
//         //     printf("blockIdx.x %d, threadIdx %d, value: %f value: %f\n", blockIdx.x, threadIdx.x, q_decay_rv.data[0][0], q_decay_rv.data[1][0]);
//         // }
//         // if (blockIdx.x == 5 && blockIdx.y == 0) {
//         //     printf("blockIdx.x %d, threadIdx %d, value: %f value: %f\n", blockIdx.x, threadIdx.x, q_decay_rv.data[0][0], q_decay_rv.data[1][0]);
//         // }
        
// #endif
//     }
//     __builtin_amdgcn_sched_barrier(0);
//     asm volatile("s_waitcnt lgkmcnt(0)");
//     asm volatile("s_waitcnt vmcnt(0)");
//     __builtin_amdgcn_sched_barrier(0);
//     __builtin_amdgcn_s_barrier();

//     if (warpid == 1) {
//         wg_arange(k_decay);
//         // warpgroup::mul(k_decay, k_decay, -1.0f); 
//         // warpgroup::add(k_decay, k_decay, CHUNK_SIZE);
//         // warpgroup::mul(k_decay, k_decay, -1.0f * slope);
//         // warpgroup::exp(k_decay, k_decay);
//         // mul(k_decay, k_decay, -1.0f); 
//         // add(k_decay, k_decay, CHUNK_SIZE);
//         // mul(k_decay, k_decay, -1.0f * slope);
//         // exp(k_decay, k_decay);

//         asm volatile("s_waitcnt lgkmcnt(0)" ::: "memory"); // 等待所有共享内存操作完成 (必加)
//         __builtin_amdgcn_s_barrier();                      // block内全线程栅栏同步 (必加)

//         load(k_decay_rv, k_decay);
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//         // if (blockIdx.x == 5 && blockIdx.y == 0) {
//             // printf("threadIdx.x %d\n", threadIdx.x);
//             for (int i = 0; i < 8; i++) {
//                 if (threadIdx.x == 64 && threadIdx.y == 0) {
//                     printf("k_decay[%d] %f\n", i, k_decay.data[i]);
//                 }
//             }
//             for (int i = CHUNK_SIZE - 8; i < CHUNK_SIZE; i++) {
//                 if (threadIdx.x == 64 && threadIdx.y == 0) {
//                     printf("k_decay[%d] %f\n", i, k_decay.data[i]);
//                 }
//             }
//         }
// #endif
//         // mul(k_decay_rv, k_decay_rv, -1.0f); 
//         // add(k_decay_rv, k_decay_rv, CHUNK_SIZE);
//         // mul(k_decay_rv, k_decay_rv, -1.0f * slope);
//         // exp(k_decay_rv, k_decay_rv);
// #ifdef DEBUG
//         // if (blockIdx.x == 0 && blockIdx.y == 0) {
//         if (blockIdx.x == 5 && blockIdx.y == 0) {
//             if (threadIdx.x == 64 && threadIdx.y == 0) {
//                 printf("k_decay_rv outer_dim %d inner_dim %d elements_per_thread %d packing %d\n",
//                     k_decay_rv.outer_dim, k_decay_rv.inner_dim, k_decay_rv.elements_per_thread, k_decay_rv.packing); // 2, 8, 16, 2
//                 printf("slope %f\n", slope);
//             }
//         }
        
//         // if (blockIdx.x == 0 && blockIdx.y == 0) {
//         //     // packed type, bf16_2
//         //     printf("blockIdx.x %d, threadIdx %d, k_decay_rv.data[0][0-7] value: (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f)\n",
//         //         blockIdx.x, threadIdx.x,
//         //         k_decay_rv.data[0][0].x, k_decay_rv.data[0][0].y, k_decay_rv.data[0][1].x, k_decay_rv.data[0][1].y, 
//         //         k_decay_rv.data[0][2].x, k_decay_rv.data[0][2].y, k_decay_rv.data[0][3].x, k_decay_rv.data[0][3].y, 
//         //         k_decay_rv.data[0][4].x, k_decay_rv.data[0][4].y, k_decay_rv.data[0][5].x, k_decay_rv.data[0][5].y, 
//         //         k_decay_rv.data[0][6].x, k_decay_rv.data[0][6].y, k_decay_rv.data[0][7].x, k_decay_rv.data[0][7].y);
//         //     printf("blockIdx.x %d, threadIdx %d, k_decay_rv.data[1][0-7] value: (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f) (%f, %f)\n",
//         //         blockIdx.x, threadIdx.x,
//         //         k_decay_rv.data[1][0].x, k_decay_rv.data[1][0].y, k_decay_rv.data[1][1].x, k_decay_rv.data[1][1].y, 
//         //         k_decay_rv.data[1][2].x, k_decay_rv.data[1][2].y, k_decay_rv.data[1][3].x, k_decay_rv.data[1][3].y, 
//         //         k_decay_rv.data[1][4].x, k_decay_rv.data[1][4].y, k_decay_rv.data[1][5].x, k_decay_rv.data[1][5].y, 
//         //         k_decay_rv.data[1][6].x, k_decay_rv.data[1][6].y, k_decay_rv.data[1][7].x, k_decay_rv.data[1][7].y);
//         // }
//         // if (blockIdx.x == 5 && blockIdx.y == 0) {
//         //     // printf("blockIdx.x %d, threadIdx %d, k_decay_rv value: %f value: %f\n", blockIdx.x, threadIdx.x, k_decay_rv.data[0][0], k_decay_rv.data[1][0]);
//         //     printf("blockIdx.x %d, threadIdx %d, k_decay_rv value: %f value: %f\n", blockIdx.x, threadIdx.x, k_decay_rv.data[0][0].x, k_decay_rv.data[0][0].y);
//         // }
//     }
// #endif

// #ifdef DEBUG
//     if (blockIdx.x == 0 && blockIdx.y == 0) {
//         for (int i = 0; i < 8; i++) {
//             if (threadIdx.x == 0 && threadIdx.y == 0){
//                 printf("before global->smem load, kv_state_smem %f\n", float(kv_state_smem.data[i]));
//             }
//         }
//     }
// #endif

//     for (int block = 0; block < blocks; block++) {
//         zero(o_reg);
//         zero(o_inter);
//         // // Load Q, K, V tiles from global memory to shared memory
//         G::load<1, false>(q_smem[tic], globals.Qg, {batch_idx, block, head_idx, 0}, swizzled_offsets_Q);
// #ifdef DEBUG
//     __builtin_amdgcn_s_waitcnt(0);
//     __builtin_amdgcn_sched_barrier(0);
//     __builtin_amdgcn_s_barrier();
//     if (blockIdx.x == 0 && blockIdx.y == 0) {
//         // for (int i = 0; i < 8; i++) {
//         //     if (threadIdx.x == 0 && threadIdx.y == 0){
//         //         printf("after q global->smem load, kv_state_smem %f\n", float(kv_state_smem.data[i]));
//         //     }
//         // }
//         if (threadIdx.x == 0 && threadIdx.y == 0) {
//             float kv_state_sum = 0.0f;
//             for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                 kv_state_sum += float(kv_state_smem.data[i]);
//             }
//             printf("after q global->smem load, kv_state_sum %f\n", kv_state_sum);
//         }
//     }
// #endif
//         G::load<1, false>(k_smem[tic], globals.Kg, {batch_idx, block, head_idx, 0}, swizzled_offsets_K);
// #ifdef DEBUG
//     __builtin_amdgcn_s_waitcnt(0);
//     __builtin_amdgcn_sched_barrier(0);
//     __builtin_amdgcn_s_barrier();
//     if (blockIdx.x == 0 && blockIdx.y == 0) {
//         // for (int i = 0; i < 8; i++) {
//         //     if (threadIdx.x == 0 && threadIdx.y == 0){
//         //         printf("after k global->smem load, kv_state_smem %f\n", float(kv_state_smem.data[i]));
//         //     }
//         // }
//         if (threadIdx.x == 0 && threadIdx.y == 0) {
//             float kv_state_sum = 0.0f;
//             for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                 kv_state_sum += float(kv_state_smem.data[i]);
//             }
//             printf("after k global->smem load, kv_state_sum %f\n", kv_state_sum);
//         }
//     }
// #endif
//         G::load<1, false>(v_smem[tic], globals.Vg, {batch_idx, block, head_idx, 0}, swizzled_offsets_V);
// #ifdef DEBUG
//     __builtin_amdgcn_s_waitcnt(0);
//     __builtin_amdgcn_sched_barrier(0);
//     __builtin_amdgcn_s_barrier();
//     if (blockIdx.x == 0 && blockIdx.y == 0) {
//         // for (int i = 0; i < 8; i++) {
//         //     if (threadIdx.x == 0 && threadIdx.y == 0){
//         //         printf("after v global->smem load, kv_state_smem %f\n", float(kv_state_smem.data[i]));
//         //     }
//         // }
//         if (threadIdx.x == 0 && threadIdx.y == 0) {
//             float kv_state_sum = 0.0f;
//             for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                 kv_state_sum += float(kv_state_smem.data[i]);
//             }
//             printf("after v global->smem load, kv_state_sum %f\n", kv_state_sum);
//         }
//     }
// #endif
//         // Below 3 loads work...
//         // load<1, q_tile<ATTN_F, bf16>, _gl_QKVO>(q_reg, globals.Qg, {batch_idx, block, head_idx, 0});
//         // load<1, k_tile<ATTN_F, bf16>, _gl_QKVO>(k_reg, globals.Kg, {batch_idx, block, head_idx, 0});
//         // load<1, v_tile<ATTN_D, bf16, col_l, rt_16x32_4_s>, _gl_QKVO>(v_reg, globals.Vg, {batch_idx, block, head_idx, 0});        

//         __builtin_amdgcn_s_waitcnt(0);
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();

//         // smem to reg
//         load(q_reg, q_smem[tic]);
//         load(k_reg, k_smem[tic]);
//         load(v_reg, v_smem[tic]);
//         __builtin_amdgcn_sched_barrier(0);
//         asm volatile("s_waitcnt lgkmcnt(0)");
//         asm volatile("s_waitcnt vmcnt(0)");
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 float kv_state_sum = 0.0f;
//                 for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                     kv_state_sum += float(kv_state_smem.data[i]);
//                 }
//                 printf("after load qkv reg, kv_state_sum %f\n", kv_state_sum);
//             }
//         }
// #endif

//         // calculation QK
//         zero(attn_block[tic]);
//         transpose(q_reg_transposed, q_reg);
//         transpose(k_reg_transposed, k_reg);
//         mma_AtB(attn_block[0], k_reg_transposed, q_reg_transposed, attn_block[0]);

//         __builtin_amdgcn_sched_barrier(0);
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 float kv_state_sum = 0.0f;
//                 for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                     kv_state_sum += float(kv_state_smem.data[i]);
//                 }
//                 printf("after QK mma, kv_state_sum %f\n", kv_state_sum);
//             }
//         }
// #endif
//         // apply diag decay
//         // TODO
//         const int lane_id = threadIdx.x % 64;
//         // const int warp_id = threadIdx.x / 64;
//         // auto get_scale = [](int i, int j){
//         //     // float ret = __expf(-(i-j) * slope);
//         //     // if (i < j) ret *= 0;

//         //     // As attn_block is Q^TK, not QK^T, let's exchange i and j.
//         //     float ret = __expf(-(j-i) * slope);
//         //     if (j < i) ret *= 0;
//         //     return ret;
//         // };
//         // constexpr int cal_num___64 = 2 * 2 * 16;
//         for (int tr = 0; tr < attn_block[0].height; tr++){
//             // for(int tc = 0; tc < 2; tc++){
//             for (int tc = 0; tc < attn_block[0].width; tc++){
//                 for (int x = 0; x < attn_block[0].packed_per_base_tile; x++){
//                     const float scale_x = get_scale(lane_id % 32 + tr * 32, lane_id / 32 * 16 + x * 2, slope);
//                     const float scale_y = get_scale(lane_id % 32 + tr * 32, lane_id / 32 * 16 + x * 2 + 1, slope);
//                     attn_block[0].tiles[tr][tc].data[x].x *= scale_x;
//                     attn_block[0].tiles[tr][tc].data[x].x *= scale_y;
//                 }
//             }
//         }
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 float kv_state_sum = 0.0f;
//                 for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                     kv_state_sum += float(kv_state_smem.data[i]);
//                 }
//                 printf("after diag_decay, kv_state_sum %f\n", kv_state_sum);
//             }
//         }
// #endif

//         // how to copy 64x64 fp32 attn_block to 64x64 bf16 attn_block_bf16?
//         // TODO
//         // attn_block_bf16 [CHUNK_SIZE, CHUNK_SIZE], 64x64
//         copy(subtile_inplace<32>(attn_block_bf16, 0), subtile_inplace<32>(attn_block[0], 0));
//         copy(subtile_inplace<32>(attn_block_bf16, 1), subtile_inplace<32>(attn_block[0], 1));
//         // attn_block_bf16_in = *reinterpret_cast<attn_tile<ATTN_D, bf16, col_l, rt_16x32_4_s>*>(&attn_block_bf16);
//         asm volatile("s_waitcnt lgkmcnt(0)");
//         asm volatile("s_waitcnt vmcnt(0)");
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
//         __builtin_amdgcn_sched_barrier(0);
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 float kv_state_sum = 0.0f;
//                 for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                     kv_state_sum += float(kv_state_smem.data[i]);
//                 }
//                 printf("after attn_block copy, kv_state_sum %f\n", kv_state_sum);
//             }
//         }
// #endif

//         // calculate AV
//         // v_reg [CHUNK_SIZE, ATTN_D], 64x128
//         // o_reg [ATTN_D, CHUNK_SIZE], 128x64
//         // v_reg * attn_block_bf16 -> o_reg, 64x128^T * 64x64 = 128x64
//         // mma_AtB(o_reg, v_reg, attn_block_bf16_in, o_reg); // 这个api对B的shape有要求?确定？，16x32
//         mma_AtB(o_reg, v_reg, attn_block_bf16, o_reg); // o_reg is o_intra

//         __builtin_amdgcn_s_setprio(0);
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
//         __builtin_amdgcn_sched_barrier(0);
// #ifdef DEBUG
//         __builtin_amdgcn_s_waitcnt(0);
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             // for (int i = 0; i < 8; i++) {
//             //     if (threadIdx.x == 0 && threadIdx.y == 0){
//             //         printf("after v global->smem load, kv_state_smem %f\n", float(kv_state_smem.data[i]));
//             //     }
//             // }
//             if (threadIdx.x == 0 && threadIdx.y == 0) {
//                 float kv_state_sum = 0.0f;
//                 for (int i = 0; i < ATTN_F*ATTN_D; i++) {
//                     kv_state_sum += float(kv_state_smem.data[i]);
//                     uint16_t val_bits = *reinterpret_cast<uint16_t*>(&kv_state_smem.data[i]);
//                     bool is_all_zero = (val_bits == 0); // 所有bit位全0 → true
//                     if (!is_all_zero) {
//                         printf("non-zero value found in kv_state_smem\n");
//                     }
//                 }
//                 printf("before local_kv_reg, kv_state_sum %f\n", kv_state_sum);
//             }
//         }
// #endif
//         // O inter
//         // q_reg * KV_state_sm
//         load(local_kv_reg, kv_state_smem);
// #ifdef DEBUG
//         if (blockIdx.x == 0 && blockIdx.y == 0) {
//             // tile[0][0]
//             for (int i = 0; i < 4; i++) {
//                 if (threadIdx.x == 0 && threadIdx.y == 0) {
//                     printf("local_kv_reg height %d width %d\n", local_kv_reg.height, local_kv_reg.width); // 8, 4
//                     printf("local_kv_reg.tiles[0][0].data length: %zu\n", sizeof(local_kv_reg.tiles[0][0].data)/sizeof(float));
//                     float temp = __bfloat162float((local_kv_reg.tiles[0][0].data[i].x)); // HIP_vector_type<float, 2>
//                     printf("local_kv_reg.tiles[0][0].data[%d].x: %f\n", i, temp);
//                     // uint16_t val_bits = *reinterpret_cast<uint16_t*>(&local_kv_reg.tiles[0][0].data[i].x);
//                     dump_bits(&local_kv_reg.tiles[0][0].data[i].x);
//                     temp = __bfloat162float((local_kv_reg.tiles[0][0].data[i].y));
//                     printf("local_kv_reg.tiles[0][0].data[%d].y: %f\n", i, temp);
//                 }
//             }
//             for (int i = 0; i < 8; i++) {
//                 if (threadIdx.x == 0 && threadIdx.y == 0){
//                     printf("kv_state_smem %f\n", float(kv_state_smem.data[i]));
//                 }
//             }
//         }
// #endif
//         mma_AtB(o_inter, local_kv_reg, q_reg_transposed, o_inter); // q_reg_transposed [ATTN_F, CHUNK_SIZE], 128x64, o_inter is (KV)^T*Q^T
//         // mul_col(o_inter, o_inter, q_decay_rv); // currently commented out for o_inter debug dump

//         // update KV state
//         auto kv_subtile_0 = subtile_inplace<ATTN_F / 2, ATTN_D>(kv_state_smem, {0, 0});
//         auto kv_subtile_1 = subtile_inplace<ATTN_F / 2, ATTN_D>(kv_state_smem, {1, 0});
        
//         // auto k_subtile_0 = subtile_inplace<CHUNK_SIZE, ATTN_F / 2>(k_smem[tic], {0, 0});
//         // auto k_subtile_1 = subtile_inplace<CHUNK_SIZE, ATTN_F / 2>(k_smem[tic], {1, 0});
        
//         // rt_fl<ATTN_F/2, ATTN_D> local_kv_0;
//         // rt_fl<ATTN_F/2, ATTN_D> local_kv_1;
//         // rt_fl<CHUNK_SIZE, ATTN_F/2> local_k_0;
//         // rt_fl<CHUNK_SIZE, ATTN_F/2> local_k_1;
//         rt_fl<ATTN_F, ATTN_D, col_l, rt_32x32_s> local_kv;      // 128x128
//         rt_fl<CHUNK_SIZE, ATTN_F, col_l, rt_32x32_s> local_k;   // 64x128
//         rt_bf<CHUNK_SIZE, ATTN_F, col_l, rt_32x32_s> local_k_bf16;   // 64x128
//         load(local_k, k_smem[0]);
//         mul_row(local_k, local_k, k_decay_rv);
//         copy(local_k_bf16, local_k);
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
//         __builtin_amdgcn_sched_barrier(0);

//         float block_decay = __expf(-slope * static_cast<float>(CHUNK_SIZE));
//         load(local_kv, kv_state_smem);
//         mul(local_kv, local_kv, block_decay);
//         // D: 128x128 float
//         // A: k_reg_transposed [ATTN_F, CHUNK_SIZE], 128x64, bf16
//         // A: k_reg [CHUNK_SIZE, ATTN_F], 64x128, bf16
//         // A: local_k_bf16, [CHUNK_SIZE, ATTN_F], 64x128, bf16
//         // B: [CHUNK_SIZE, ATTN_D], 64x128
//         mma_AtB(local_kv, local_k_bf16, v_reg, local_kv);
//         __builtin_amdgcn_s_setprio(0);
//         __builtin_amdgcn_sched_barrier(0);
//         __builtin_amdgcn_s_barrier();
//         __builtin_amdgcn_sched_barrier(0);
        
//         // store updated kv state
//         // TODO
//         // store(kv_state_smem, local_kv);

//         // o_intra + o_inter
//         // o_reg [ATTN_D, CHUNK_SIZE], 128x64, o_reg is o_intra
//         // o_tile_transpose<ATTN_D, float, col_l, rt_32x32_s> o_inter;  [ATTN_D, CHUNK_SIZE]
//         add(o_reg, o_inter, o_reg);


        
//         o_tile<ATTN_D, float, row_l, rt_32x32_s> o_reg_transposed; // [CHUNK_SIZE, ATTN_D]
//         // o_tile<ATTN_D, float, col_l, rt_32x32_s> o_reg_transposed;
//         transpose(o_reg_transposed, o_reg);
//         // add(o_reg_transposed, o_reg_transposed, o_inter);
//         // store<1>(globals.Og, o_reg_transposed, {batch_idx, 0, head_idx, 0});
//         store<1>(globals.Og, o_reg_transposed, {batch_idx, block, head_idx, 0});


//         // debug dump
//         // dump o_inter, o_tile<ATTN_D, float, col_l, rt_32x32_s> o_inter; [CHUNK_SIZE, ATTN_D]
//         o_tile<ATTN_D, float, row_l, rt_32x32_s> o_inter_transposed;
//         transpose(o_inter_transposed, o_inter);
//         store<1>(globals.ODEBUGg, o_inter_transposed, {batch_idx, block, head_idx, 0});
//     }

//     // o_tile<ATTN_D, float, row_l, rt_32x32_s> o_reg_transposed;
//     // transpose(o_reg_transposed, o_reg);
//     // store<1>(globals.Og, o_reg_transposed, {batch_idx, 0, head_idx, 0});
}

lightning_attn2_globals lightning_attn2_init(
    bf16 *d_q, bf16 *d_k, bf16 *d_v, bf16 *d_o,
    // debug dump output,
    bf16 *d_debug_o,
    float *d_slopes,
    // int B, int H, int N, int ATTN_F, int ATTN_D
    unsigned long B, unsigned long H, unsigned long N
) {
    // global pointers. 
    // using q_tile       = st_bf<CHUNK_SIZE,   ATTN_F>;
    // using k_tile       = st_bf<CHUNK_SIZE,   ATTN_F>;
    // using k_split_tile = st_bf<CHUNK_SIZE,   ATTN_F/2>;
    // using v_tile       = st_bf<CHUNK_SIZE,   ATTN_D>;
    // using o_tile       = st_bf<CHUNK_SIZE,   ATTN_D>;
    
    // using q_global       = gl<bf16, -1, -1, -1, -1, q_tile>;
    // using k_global       = gl<bf16, -1, -1, -1, -1, k_tile>;
    // using k_split_global = gl<bf16, -1, -1, -1, -1, k_split_tile>;
    // using v_global       = gl<bf16, -1, -1, -1, -1, v_tile>;
    // using o_global       = gl<bf16, -1, -1, -1, -1, o_tile>;

    using globals = lightning_attn2_globals;
    // q_global             q_arg{d_q, B, H, N, ATTN_F};
    // k_global             k_arg{d_k, B, H, N, ATTN_F};
    // k_split_global k_split_arg{d_k, B, H, N, ATTN_F}; 
    // v_global             v_arg{d_v, B, H, N, ATTN_D};
    // o_global             o_arg{d_o, B, H, N, ATTN_D};

    // _gl_QKVO             q_arg{d_q, B, H, N, ATTN_F};
    // _gl_QKVO             k_arg{d_k, B, H, N, ATTN_F};
    // _gl_QKVO             k_split_arg{d_k, B, H, N, ATTN_F}; 
    // _gl_QKVO             v_arg{d_v, B, H, N, ATTN_D};
    // _gl_QKVO             o_arg{d_o, B, H, N, ATTN_D};
    _gl_QKVO             q_arg{d_q, B, N, H, ATTN_F__128};
    _gl_QKVO             k_arg{d_k, B, N, H, ATTN_F__128};
    _gl_QKVO             k_split_arg{d_k, B, N, H, ATTN_F__128}; 
    _gl_QKVO             v_arg{d_v, B, N, H, ATTN_D__128};
    _gl_QKVO             o_arg{d_o, B, N, H, ATTN_D__128};

    // debug dump
    _gl_QKVO            odebug_arg{d_debug_o, B, N, H, ATTN_D__128};

    globals g{
        q_arg, k_arg, k_split_arg, v_arg, o_arg,
        // debug dump
        odebug_arg,
        // d_slopes
        reinterpret_cast<uintptr_t>(d_slopes)
    };

    return g;
}

// void dispatch_micro(lightning_attn2_globals g) {
//     //temp test
//     float* dev_ptr = reinterpret_cast<float*>(g.slopes);

//     unsigned long mem_size = g.dynamic_shared_memory();
//     hipFuncSetAttribute((void*)lightning_attn2_kernel, hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
//     lightning_attn2_kernel<<<g.grid(), g.block(), mem_size, g.stream>>>(g, ATTN_N);
// }

// PYBIND11_MODULE(tk_kernel, m) {
//     m.doc() = "tk_kernel python module";
//     py::bind_function<dispatch_micro>(m, "dispatch_micro", 
//         &lightning_attn2_globals::Qg, 
//         &lightning_attn2_globals::Kg,
//         &lightning_attn2_globals::K_split_g, 
//         &lightning_attn2_globals::Vg, 
//         &lightning_attn2_globals::Og,
//         &lightning_attn2_globals::slopes
//     );
// }

#define HipCheckError()    __hipCheckError( __FILE__, __LINE__ )
inline void __hipCheckError( const char *file, const int line ) {
    hipError_t err = hipGetLastError();
    if ( hipSuccess != err ) {
        fprintf( stderr, "hipCheckError() failed at %s:%i : %s\n",
                 file, line, hipGetErrorString( err ) );
        exit( -1 );
    }
    err = hipDeviceSynchronize();
    if( hipSuccess != err ) {
        fprintf( stderr, "hipCheckError() with sync failed at %s:%i : %s\n",
                 file, line, hipGetErrorString( err ) );
        exit( -1 );
    }
}

int main(int argc, char **argv) {
    // constexpr int B = 16;
    // constexpr int D = 128;
    // constexpr int H = 8;
    // constexpr int F = 128;
    // constexpr int N = 1024;
    constexpr int B = 1;
    constexpr int D = 128;
    constexpr int H = 1;
    constexpr int F = 128;
    constexpr int N = 64;
    // constexpr int N = 128;

    constexpr int warmup_iters = 1;
    constexpr int timing_iters = 1;

    int TOTAL_ELEMENTS_QK = B * H * N * D;
    int TOTAL_ELEMENTS_VO = B * H * N * F;

    float *slopes      = new float[H];
    float *q           = new float[TOTAL_ELEMENTS_QK];
    float *k           = new float[TOTAL_ELEMENTS_QK];
    float *v           = new float[TOTAL_ELEMENTS_VO];
    float *o_ref       = new float[TOTAL_ELEMENTS_VO];
    float *o           = new float[TOTAL_ELEMENTS_VO];
    
    bf16 *q_bf        = new bf16[TOTAL_ELEMENTS_QK];
    bf16 *k_bf        = new bf16[TOTAL_ELEMENTS_QK];
    bf16 *v_bf        = new bf16[TOTAL_ELEMENTS_VO];
    bf16 *o_bf        = new bf16[TOTAL_ELEMENTS_VO];

    // debug dump
    int TOTAL_ELEMENTS_DEBUGO = B * H * N * D;
    float *o_debug_ref = new float[TOTAL_ELEMENTS_DEBUGO];
    float *o_debug = new float[TOTAL_ELEMENTS_DEBUGO];
    bf16 *o_debug_bf  = new bf16[TOTAL_ELEMENTS_DEBUGO];
    std::cout << "TOTAL_ELEMENTS_DEBUGO " << TOTAL_ELEMENTS_DEBUGO << std::endl;


    if (argc > 1) {
        std::ifstream infile(argv[1]);
        std::cout << "Reading input file: " << argv[1] << std::endl;

        // 1. Read slopes
        for(int i = 0; i < ATTN_H; i++) {
            infile >> slopes[i];
            printf("slopes[%d] = %f\n", i, slopes[i]);
        }
        std::cout << "Finished loading " << ATTN_H << " slopes" << std::endl;

        // 2. Read Q
        for(int i = 0; i < TOTAL_ELEMENTS_QK; i++) infile >> q[i];
        std::cout << "Finished loading " << TOTAL_ELEMENTS_QK << " elements of Q" << std::endl;

        // 3. Read K
        for(int i = 0; i < TOTAL_ELEMENTS_QK; i++) infile >> k[i];
        std::cout << "Finished loading " << TOTAL_ELEMENTS_QK << " elements of K" << std::endl;

        // 4. Read V
        for(int i = 0; i < TOTAL_ELEMENTS_VO; i++) infile >> v[i];
        std::cout << "Finished loading " << TOTAL_ELEMENTS_VO << " elements of V" << std::endl;

        // 5. Read O reference
        for(int i = 0; i < TOTAL_ELEMENTS_VO; i++) infile >> o_ref[i];
        std::cout << "Finished loading " << TOTAL_ELEMENTS_VO << " elements of O_REF" << std::endl;

        // read debug dump
        for(int i = 0; i < TOTAL_ELEMENTS_DEBUGO; i++) infile >> o_debug_ref[i];
        std::cout << "Finished loading " << TOTAL_ELEMENTS_DEBUGO << " elements of O_DEBUG_REF" << std::endl;
    }

    // Convert to bf16
    for(uint64_t i = 0; i < TOTAL_ELEMENTS_QK; i++) {
        q_bf[i] = __float2bfloat16(q[i]);
        k_bf[i] = __float2bfloat16(k[i]);
    }
    for(uint64_t i = 0; i < TOTAL_ELEMENTS_VO; i++) {
        v_bf[i] = __float2bfloat16(v[i]);
    }
    
    bf16 *d_q, *d_k, *d_v, *d_o;
    bf16 *d_debug_o; // used for debug dump only
    float *d_slopes;
    
    hipMalloc(&d_slopes,   H            * sizeof(float));
    hipMalloc(&d_q,        TOTAL_ELEMENTS_QK * sizeof(bf16));
    hipMalloc(&d_k,        TOTAL_ELEMENTS_QK * sizeof(bf16));
    hipMalloc(&d_v,        TOTAL_ELEMENTS_VO * sizeof(bf16));
    hipMalloc(&d_o,        TOTAL_ELEMENTS_VO * sizeof(bf16));

    hipMemcpy(d_slopes, slopes,   H            * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_q,      q_bf,     TOTAL_ELEMENTS_QK * sizeof(bf16),  hipMemcpyHostToDevice);
    hipMemcpy(d_k,      k_bf,     TOTAL_ELEMENTS_QK * sizeof(bf16),  hipMemcpyHostToDevice);
    hipMemcpy(d_v,      v_bf,     TOTAL_ELEMENTS_VO * sizeof(bf16),  hipMemcpyHostToDevice);
    
    // zero out d_o
    hipMemset(d_o, 0, TOTAL_ELEMENTS_VO * sizeof(bf16));

    // debug dump
    hipMalloc(&d_debug_o, TOTAL_ELEMENTS_DEBUGO * sizeof(bf16));
    hipMemset(d_debug_o, 0, TOTAL_ELEMENTS_DEBUGO * sizeof(bf16));

    hipDeviceSynchronize();
    HipCheckError();

    // Set up kernel configuration
    unsigned long mem_size = kittens::MAX_SHARED_MEMORY; 

    // Initialize kernel configuration
    lightning_attn2_globals g = lightning_attn2_init(
        d_q, d_k, d_v, d_o,
        // debug dump
        d_debug_o,
        d_slopes,
        B, H, N
    );

    hipFuncSetAttribute(
        (void*)lightning_attn2_kernel,
        hipFuncAttributeMaxDynamicSharedMemorySize,
        mem_size
    );

    // Run kernel
    const int ITER = 1;
    hipDeviceSynchronize();
    HipCheckError();

    std::cout << "Starting kernel with " << B * H << " blocks and " << NUM_THREADS << " threads\n";
    float avg_us = 0;
    for(int i = 0; i < ITER; i++) {
        // zero out d_o
        hipMemset(d_o, 0, TOTAL_ELEMENTS_VO * sizeof(bf16));
        // debug dump
        hipMemset(d_debug_o, 0, TOTAL_ELEMENTS_DEBUGO * sizeof(bf16));
        hipDeviceSynchronize();
        HipCheckError();

        const auto start = std::chrono::high_resolution_clock::now();
        // lightning_attn2_kernel<<<dim3(H,B), NUM_THREADS, mem_size>>>(g, N);
        lightning_attn2_kernel<<<g.grid(), g.block(), mem_size>>>(g, N);
        hipDeviceSynchronize();
        const auto finish = std::chrono::high_resolution_clock::now();
        HipCheckError();
        avg_us += std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count();
    }
    avg_us /= ITER;
    std::cout << "Average execution time: " << avg_us << " us" << std::endl;

    // Copy results back and compare
    hipMemcpy(o_bf, d_o, TOTAL_ELEMENTS_VO * sizeof(bf16), hipMemcpyDeviceToHost);
    
    // Convert output to float
    for(int i = 0; i < TOTAL_ELEMENTS_VO; i++) {
        o[i] = __bfloat162float(o_bf[i]);
    }

    // debug dump
    hipMemcpy(o_debug_bf, d_debug_o, TOTAL_ELEMENTS_DEBUGO * sizeof(bf16), hipMemcpyDeviceToHost);
    for(int i = 0; i < TOTAL_ELEMENTS_DEBUGO; i++) {
        o_debug[i] = __bfloat162float(o_debug_bf[i]);
    }
    std::ofstream o_debug_ref_file("printouts/o_debug_ref.txt");
    std::ofstream o_debug_file("printouts/o_debug.txt");
    std::ofstream debug_diff_file("printouts/debug_sdiff.txt");
    float max_debug_diff = 0, total_debug_diff = 0, total_debug_abs = 0;
    for(int i = 0; i < TOTAL_ELEMENTS_DEBUGO; i++) {
        float debug_diff = o_debug[i] - o_debug_ref[i];
        if (i < 8) {
            std::cout << "o_debug[" << i << "] = " << o_debug[i]
              << " o_debug_ref[" << i << "] = " << o_debug_ref[i] << std::endl;
        }
        // else {
        //     std::cout << "o_debug[" << i << "] = " << o_debug[i]
        //       << " o_debug_ref[" << i << "] = " << o_debug_ref[i] << std::endl;
        // }
        
        o_debug_ref_file << o_debug_ref[i] << ' ';
        o_debug_file << o_debug[i] << ' ';
        debug_diff_file << debug_diff << ' ';
        
        if(i % 64 == 63) {
            o_debug_ref_file << std::endl;
            o_debug_file << std::endl;
            debug_diff_file << std::endl;
        }

        if(abs(debug_diff) > max_debug_diff || std::isnan(debug_diff)) {
            max_debug_diff = abs(debug_diff);
            if(std::isnan(debug_diff)) {
                printf("NAN detected idx=%d, o_debug = %f, o_debug_ref = %f, debug_diff = %f\n", i, o_debug[i], o_debug_ref[i], debug_diff);
                break;
            }
        }

        total_debug_abs += abs(o_debug_ref[i]);
        total_debug_diff += abs(debug_diff);
    }
    // Print error metrics
    std::cout.setf(std::ios::fixed, std::ios::floatfield);
    std::cout.precision(6);
    std::cout.width(12);
    std::cout << "total_debug_diff=" << total_debug_diff
              << "total_debug_abs=" << total_debug_abs << std::endl;
    std::cout << "O | avg_diff=" << (total_debug_diff/TOTAL_ELEMENTS_DEBUGO) 
              << ", avg_abs=" << (total_debug_abs/TOTAL_ELEMENTS_DEBUGO)
              << ", rel_diff=" << 100*(total_debug_diff/total_debug_abs) 
              << "%, max_debug_diff=" << max_debug_diff << std::endl;




    // Write results to files for analysis
    std::ofstream o_ref_file("printouts/o_ref.txt");
    std::ofstream o_file("printouts/o.txt");
    std::ofstream diff_file("printouts/diff.txt");

    float max_diff = 0, total_diff = 0, total_abs = 0;
    for(int i = 0; i < TOTAL_ELEMENTS_VO; i++) {
        float diff = o[i] - o_ref[i];
        if (i < 8) {
            std::cout << "o[" << i << "] = " << o[i]
              << " o_ref[" << i << "] = " << o_ref[i] << std::endl;
        }
        // else {
        //     std::cout << "o[" << i << "] = " << o[i]
        //       << " o_ref[" << i << "] = " << o_ref[i] << std::endl;
        // }
        
        o_ref_file << o_ref[i] << ' ';
        o_file << o[i] << ' ';
        diff_file << diff << ' ';
        
        if(i % 64 == 63) {
            o_ref_file << std::endl;
            o_file << std::endl;
            diff_file << std::endl;
        }

        if(abs(diff) > max_diff || std::isnan(diff)) {
            max_diff = abs(diff);
            if(std::isnan(diff)) {
                printf("NAN detected idx=%d, o = %f, o_ref = %f, diff = %f\n", i, o[i], o_ref[i], diff);
                break;
            }
        }

        total_abs += abs(o_ref[i]);
        total_diff += abs(diff);
    }

    // Print error metrics
    std::cout.setf(std::ios::fixed, std::ios::floatfield);
    std::cout.precision(6);
    std::cout.width(12);
    std::cout << "total_diff=" << total_diff
              << "total_abs=" << total_abs << std::endl;
    std::cout << "O | avg_diff=" << (total_diff/TOTAL_ELEMENTS_VO) 
              << ", avg_abs=" << (total_abs/TOTAL_ELEMENTS_VO)
              << ", rel_diff=" << 100*(total_diff/total_abs) 
              << "%, max_diff=" << max_diff << std::endl;

    // Cleanup
    hipFree(d_q);
    hipFree(d_k);
    hipFree(d_v);
    hipFree(d_o);
    hipFree(d_slopes);

    delete[] slopes;
    delete[] q;
    delete[] k;
    delete[] v;
    delete[] o;
    delete[] o_ref;
    delete[] q_bf;
    delete[] k_bf;
    delete[] v_bf;
    delete[] o_bf;

    // debug dump
    hipFree(d_debug_o);
    delete[] o_debug;
    delete[] o_debug_ref;
    delete[] o_debug_bf;

    return 0;
}